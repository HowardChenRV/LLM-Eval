benchmark_name: serving_perf_eval
# Inference server config 
backend: maas
url: https://qa.cloud-dev.llm-ai.com:9443/maas/v1/chat/completions
model: megrez-3b-instruct
tokenizer: /Users/howardchen/models/Megrez-3B-Instruct
api_key: sk-i5qhpbyd2otunghy
extra_args:
  max_completion_tokens: 512
  top_p: 0.95
  temperature: 0.7
  stream: False
extra_headers:
  x-llm-env: "6"

# Dataset config
dataset_name: local
dataset_path: /Users/howardchen/Downloads/a-500-2000.jsonl
num_prompts: 1

# Perf test config
perf_test_type: multiple
concurrency_list:
  - 1
request_rate_list: 
  - -1
save_data: True

test_meta:
  tester: chenyonghua
  hardware: RTX4090-24G
  hardware_num: 1
  model: Megrez-3b-Instruct
  quantization_method: default
  model_source: OPEN
  framework: vllm-nvidia
  framework_version: v3.8.4
  test_source: TEST
  # other labels
  cluster: 1实例